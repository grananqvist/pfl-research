# TODO: Update this once we're done with example for both cv and ls

data_path: ~/.cache/mlx.data/librispeech
# Which dataset to train on, currently only supports librispeech for ASR.
dataset: librispeech
# Which data splits to use
training_split: train-clean-100
validation_split: dev-clean
evaluation_splits: "dev-clean dev-other test-clean test-other"
num_threads_data_processing: 24

# Which model to train, currently only supports asr_ctc_transformer for ASR.
model_name: asr_ctc_transformer
# Which algorithm to train with: {fedavg, fedprox}.
algorithm_name: fedavg
# Model related parameters
dummy_model_size: 255

central_optimizer: adam
adaptivity_degree: 0.01
data_fraction: 1.0
central_data_fraction: 0.01
evaluation_frequency: 20

learning_rate: 0.1
cohort_size: 64
noise_cohort_size: 64
# Faster simulation. Central val is used.
val_cohort_size: 0 
central_num_iterations: 2000
batch_strategy: dynamic
local_batch_size: 384000
central_eval_batch_size: 384000
max_sample_audio_length: 384000
local_num_epochs: 10
local_learning_rate: 0.3
weighting: user
# TODO: rdar://109165296 Implement LocalLRDecay as an adaptive hyperparameter
local_lr_decay: False
# TODO: rdar://109165050 Implement DecayToFedSGD as an adaptive hyperparameter
# - fedsgd_after_amount_trained: 0.75


central_privacy_mechanism: none
central_epsilon: 2.0
central_delta: 1e-6
central_privacy_clipping_bound: 1.0
central_order: 2
population: 1e6
use_tensorboard: False
#save_model_path: './checkpoints'

#wandb_project_id: testing
# This result in all algorithm parameters being added, even though
# you only select 1 algorithm. Useful for reusing the same config
# for multiple algorithms.
add_all_arguments: true

# FedProx params from benchmark-algos/sweep-fedprox
mu: 0.001

# AdaFedProx override default params
adafedprox_metric_name: "Central val | loss"
adafedprox_adapt_frequency: 20

# SCAFFOLD override default params
# TODO: Fix
scaffold_population: 342477
